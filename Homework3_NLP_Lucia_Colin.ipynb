{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d586cd1d",
   "metadata": {},
   "source": [
    "<img src=\"logoiit.png\" width=\"200\" img style=\"float: right;\"> \n",
    "\n",
    "**NATURAL LANGUAGE PROCESSING. HOMEWORK 3.**<br>\n",
    "Author: Lucía Colín Cosano. A20552447."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feee4a3",
   "metadata": {},
   "source": [
    "**PROBLEM 1 – Reading the data**\n",
    "\n",
    "• Read in file \"train.tsv\" from the Stanford Sentiment Treebank (SST) as shared in the GLUE task. (See section \"DATA\" above.)\n",
    "\n",
    "• Next, split your dataset into train, test, and validation datasets with the sizes defined.\n",
    "\n",
    "• Review the column \"label\" which indicates positive=1 or negative=0 sentiment. What is the prior probability of each class on your training set? Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edad749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a48167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.tsv\", sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "127a78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 100\n",
    "test_size = 100\n",
    "\n",
    "validation_set = df.sample(validation_size, random_state=1)\n",
    "df = df.drop(validation_set.index)\n",
    "\n",
    "test_set = df.sample(test_size, random_state=2)\n",
    "df = df.drop(test_set.index)\n",
    "\n",
    "training_set = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4671510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability of Positive Sentiment: 0.5578638550090098\n",
      "Prior Probability of Negative Sentiment: 0.4421361449909902\n"
     ]
    }
   ],
   "source": [
    "num_positive = training_set['label'].sum() \n",
    "num_negative = len(training_set) - num_positive  \n",
    "total_samples = len(training_set)\n",
    "\n",
    "prior_prob_positive = num_positive / total_samples\n",
    "prior_prob_negative = num_negative / total_samples\n",
    "\n",
    "print(\"Prior Probability of Positive Sentiment:\", prior_prob_positive)\n",
    "print(\"Prior Probability of Negative Sentiment:\", prior_prob_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2511554",
   "metadata": {},
   "source": [
    "**PROBLEM 2 – Tokenizing data**\n",
    "\n",
    "• Write a function that takes a sentence as input, represented as a string, and converts it to a tokenized sequence padded by start and end symbols. \n",
    "\n",
    "• Apply your function to all sentences in your training set. Show the tokenization of the first sentence of your training set in your notebook output.\n",
    "\n",
    "• What is the vocabulary size of your training set? Include your start and end symbol in your vocabulary. Show your result in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb0b30b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f57ba17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization of the first sentence:\n",
      "hide new secretions from the parental units \n",
      "['<s>', 'hide', 'new', 'secretions', 'from', 'the', 'parental', 'units', '</s>']\n"
     ]
    }
   ],
   "source": [
    "training_set['tokenized_sentence'] = training_set['sentence'].apply(tokenize_sentence)\n",
    "\n",
    "unique_tokens = set()\n",
    "\n",
    "# Collect unique tokens from all tokenized sentences\n",
    "for tokens in training_set['tokenized_sentence']:\n",
    "    unique_tokens.update(tokens)\n",
    "\n",
    "# Tokenize the first sentence in your training set\n",
    "first_sentence = training_set['sentence'].iloc[0]\n",
    "tokenized_first_sentence = tokenize_sentence(first_sentence)\n",
    "\n",
    "# Display the tokenization of the first sentence\n",
    "print(\"Tokenization of the first sentence:\")\n",
    "print(first_sentence)\n",
    "print(tokenized_first_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db8baeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 14802\n"
     ]
    }
   ],
   "source": [
    "# Include start and end symbols in the vocabulary\n",
    "unique_tokens.add('<s>')\n",
    "unique_tokens.add('</s>')\n",
    "\n",
    "# Calculate the vocabulary size\n",
    "vocab_size = len(unique_tokens)\n",
    "\n",
    "# Display the vocabulary size\n",
    "print(\"Vocabulary Size:\", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c08edf",
   "metadata": {},
   "source": [
    "**PROBLEM 3 – Bigram counts**\n",
    "\n",
    "• Write a function that takes an array of tokenized sequences as input (i.e., a list of lists) and counts bigram frequencies in that dataset. Your function should return a two-level dictionary (dictionary of dictionaries) or similar data structure, where the value at index [wi][wj] gives the frequency count of bigram (wi, wj). For example, this expression would give the counts of the bigram \"academy award\": bigram_counts[\"academy\"][\"award\"]\n",
    "\n",
    "• Apply your function to the output of problem 2. You should build one counter that represents all sentences in the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bbe1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_bigrams(tokenized_sequences):\n",
    "    bigram_counts = {} \n",
    "\n",
    "    for tokens in tokenized_sequences:\n",
    "        for wi, wj in zip(tokens, tokens[1:]):\n",
    "            bigram_counts.setdefault(wi, {}).setdefault(wj, 0)\n",
    "            bigram_counts[wi][wj] += 1\n",
    "\n",
    "    return bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41081d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_counts = count_bigrams(training_set['tokenized_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3aa96df",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_with_the_count = bigram_counts.get(\"<s>\", {}).get(\"the\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd90761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram count of ('<s>', 'the'):  4455\n"
     ]
    }
   ],
   "source": [
    "print(\"Bigram count of ('<s>', 'the'): \", start_with_the_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4734ec4",
   "metadata": {},
   "source": [
    "**PROBLEM 4 – Smoothing**\n",
    "\n",
    "• Write a function that implements formula [6.13] in that E-NLP textbook (page 129, 6.2 Smoothing and discounting). That is, write a function that applies smoothing and returns a (negative) log-probability of a word given the previous word in the sequence. \n",
    "\n",
    "• Using this function to show the log probability that the word \"academy\" will be followed by the word \"award\". Try this with alpha=0.001 and alpha=0.5 (you should see very different results!). Show your results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "703d886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_smoothed_log_prob(wm, wm_1, bigram_counts, alpha, vocab_size):\n",
    "    count_wm_1_wm = bigram_counts.get(wm_1, {}).get(wm, 0)\n",
    "\n",
    "    numerator = count_wm_1_wm + alpha\n",
    "    denominator = sum(bigram_counts.get(wm_1, {}).values()) + (alpha * vocab_size)\n",
    "    log_prob = math.log(numerator / denominator)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b75e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability ('academy' -> 'award') with alpha = 0.001: -1.0248273197292836\n",
      "Log Probability ('academy' -> 'award') with alpha = 0.5: -6.172171898547395\n"
     ]
    }
   ],
   "source": [
    "# Assuming you already have bigram_counts, vocab_size, and the words \"academy\" and \"award\"\n",
    "word_wm_1 = \"academy\"\n",
    "word_wm = \"award\"\n",
    "alpha_1 = 0.001\n",
    "alpha_2 = 0.5\n",
    "\n",
    "log_prob_alpha_1 = calculate_smoothed_log_prob(word_wm, word_wm_1, bigram_counts, alpha_1, vocab_size)\n",
    "log_prob_alpha_2 = calculate_smoothed_log_prob(word_wm, word_wm_1, bigram_counts, alpha_2, vocab_size)\n",
    "\n",
    "print(f\"Log Probability ('{word_wm_1}' -> '{word_wm}') with alpha = {alpha_1}: {log_prob_alpha_1}\")\n",
    "print(f\"Log Probability ('{word_wm_1}' -> '{word_wm}') with alpha = {alpha_2}: {log_prob_alpha_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4184b5a0",
   "metadata": {},
   "source": [
    "**PROBLEM 5 – Sentence log-probability**\n",
    "\n",
    "• Write a function that returns the log-probability of a sentence which is expected to be a negative number. To do this, assume that the probability of a word in a sequence only depends on the previous word. \n",
    "\n",
    "• Use your function to compute the log probability of these two sentences (Note that the 2nd is not natural English, so it should have a lower (more negative) result that the first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "281b29c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_log_prob(sentence, bigram_counts, alpha, vocab_size):\n",
    "    words = sentence.split()\n",
    "    log_prob = 0.0\n",
    "\n",
    "    for i in range(1, len(words)):\n",
    "        log_prob += calculate_smoothed_log_prob(words[i], words[i - 1], bigram_counts, alpha, vocab_size)\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83ab5ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability of Sentence 1: -67.79457212208044\n",
      "Log Probability of Sentence 2: -126.27667113068851\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"this was a really great movie but it was a little too long.\"\n",
    "sentence2 = \"long too little a was it but movie great really a was this.\"\n",
    "\n",
    "alpha = 0.01 \n",
    "\n",
    "log_prob1 = calculate_sentence_log_prob(sentence1, bigram_counts, alpha, vocab_size)\n",
    "log_prob2 = calculate_sentence_log_prob(sentence2, bigram_counts, alpha, vocab_size)\n",
    "\n",
    "print(\"Log Probability of Sentence 1:\", log_prob1)\n",
    "print(\"Log Probability of Sentence 2:\", log_prob2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b4586",
   "metadata": {},
   "source": [
    "With the result obtain we can check what it was meant to happen, the second sentence has a more negative log probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5fbada",
   "metadata": {},
   "source": [
    "**PROBLEM 6 – Tuning Alpha**\n",
    "\n",
    "Next, use your validation set to select a good value for \"alpha\".\n",
    "\n",
    "• Apply the function you wrote in Problem 5 to your validation dataset using 3 different values of \"alpha\", such as (0.001, 0.01, 0.1). For each value, show the log-likelihood estimate of the validation set. That is, in your notebook show the sum of the log probabilities of all sentences.\n",
    "\n",
    "• Which alpha gives you the best result? To indicate your selection to the grader, save your selected value to a variable named \"selected_alpha\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcdb052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood estimates for different alpha values:\n",
      "Alpha = 0.001: Log-likelihood = -3829.3573328360308\n",
      "Alpha = 0.01: Log-likelihood = -4286.71505268244\n",
      "Alpha = 0.1: Log-likelihood = -5217.489322163317\n",
      "Best alpha: 0.001\n"
     ]
    }
   ],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "log_likelihoods = [] \n",
    "\n",
    "for alpha in alpha_values:\n",
    "    total_log_prob = 0.0\n",
    "    for sentence in validation_set['sentence']:\n",
    "        total_log_prob += calculate_sentence_log_prob(sentence, bigram_counts, alpha, vocab_size)\n",
    "    log_likelihoods.append(total_log_prob)\n",
    "\n",
    "best_alpha = alpha_values[log_likelihoods.index(max(log_likelihoods))]\n",
    "\n",
    "print(\"Log-likelihood estimates for different alpha values:\")\n",
    "for i, alpha in enumerate(alpha_values):\n",
    "    print(f\"Alpha = {alpha}: Log-likelihood = {log_likelihoods[i]}\")\n",
    "\n",
    "print(\"Best alpha:\", best_alpha)\n",
    "selected_alpha=best_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9cc8f9",
   "metadata": {},
   "source": [
    "**PROBLEM 7 – Applying Language Models**\n",
    "\n",
    "In this problem, you will classify your test set of 100 sentences by sentiment, by applying your work from previous problems and modeling the language of both positive and negative sentiment.\n",
    "To do this, you can follow these steps:\n",
    "\n",
    "• Separate your training dataset into positive and negative sentences, and compute vocabulary size and bigram counts for both datasets.\n",
    "\n",
    "• For each of the 100 sentences in your test set:\n",
    "\n",
    "- Compute both a \"positive sentiment score\" and a \"negative sentiment score\" using (1) the function you wrote in Problem 5, (2) Bayes rule, and (3) class priors as computed in Problem 1.\n",
    "\n",
    "- Compare these scores to assign a predicted sentiment label to the sentence.\n",
    "\n",
    "• What is the class distribution of your predicted label? That is, how often did your method predict positive sentiment, correctly or incorrectly? How often did it predict negative sentiment? Show results in your notebook.\n",
    "\n",
    "• Compare your predicted label to the true sentiment label. What is the accuracy of this experiment? That is, how often did the true and predicted label match on the test set? Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a555ac96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution of Predicted Labels:\n",
      "Predicted Positive Sentiment: 56\n",
      "Predicted Negative Sentiment: 44\n",
      "Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Separate the Training Dataset into positive and negative subsets\n",
    "positive_training_set = training_set[training_set['label'] == 1]\n",
    "negative_training_set = training_set[training_set['label'] == 0]\n",
    "\n",
    "# Step 2: Compute Vocabulary Size and Bigram Counts for Both Datasets\n",
    "# Step 2: Compute Vocabulary Size and Bigram Counts for Both Datasets\n",
    "vocab_size_pos = len(set(word for tokens in positive_training_set['tokenized_sentence'] for word in tokens))\n",
    "bigram_counts_pos = count_bigrams(positive_training_set['tokenized_sentence'])\n",
    "\n",
    "vocab_size_neg = len(set(word for tokens in negative_training_set['tokenized_sentence'] for word in tokens))\n",
    "bigram_counts_neg = count_bigrams(negative_training_set['tokenized_sentence'])\n",
    "\n",
    "\n",
    "# Step 3 and 4: Compute Sentiment Scores and Predict Sentiment Labels for the Test Set\n",
    "predicted_sentiment_labels = []\n",
    "\n",
    "for sentence in test_set['sentence']:\n",
    "    log_prob_positive = calculate_sentence_log_prob(sentence, bigram_counts_pos, selected_alpha, vocab_size_pos) + math.log(prior_prob_positive)\n",
    "    log_prob_negative = calculate_sentence_log_prob(sentence, bigram_counts_neg, selected_alpha, vocab_size_neg) + math.log(prior_prob_negative)\n",
    "\n",
    "    if log_prob_positive > log_prob_negative:\n",
    "        predicted_sentiment_labels.append(1)  # Predicted positive sentiment\n",
    "    else:\n",
    "        predicted_sentiment_labels.append(0)  # Predicted negative sentiment\n",
    "\n",
    "# Step 5: Analyze the Class Distribution of Predicted Labels\n",
    "positive_predictions = sum(predicted_sentiment_labels)\n",
    "negative_predictions = len(predicted_sentiment_labels) - positive_predictions\n",
    "\n",
    "# Step 6: Calculate Accuracy\n",
    "true_labels = test_set['label'].tolist()\n",
    "correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_sentiment_labels) if true == predicted)\n",
    "accuracy = correct_predictions / len(test_set)\n",
    "\n",
    "# Print results\n",
    "print(\"Class Distribution of Predicted Labels:\")\n",
    "print(\"Predicted Positive Sentiment:\", positive_predictions)\n",
    "print(\"Predicted Negative Sentiment:\", negative_predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
