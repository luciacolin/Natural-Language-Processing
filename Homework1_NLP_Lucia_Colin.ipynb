{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80bc079e",
   "metadata": {},
   "source": [
    "<img src=\"logoiit.png\" width=\"200\" img style=\"float: right;\"> \n",
    "\n",
    "**NATURAL LANGUAGE PROCESSING. HOMEWORK 1.**<br>\n",
    "Author: Lucía Colín Cosano. A20552447."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f52fa",
   "metadata": {},
   "source": [
    "**PROBLEM 1**\n",
    "\n",
    "• Read in these two GLUE datasets (see section “DATA” above). Also convert alphabetical characters to lower case:\n",
    "\n",
    "• Convert each dataset into a single list of tokens by applying the function “word_tokenize()” in the NLTK :: nltk.tokenize package. We will use these lists represent two distributions of English text.\n",
    "\n",
    "• To show you have finished this step, print the first 10 tokens from each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24a8e37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lulac\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens from SST dataset:\n",
      "['hide', 'new', 'secretions', 'from', 'the', 'parental', 'units', 'contains', 'no', 'wit']\n",
      "\n",
      "First 10 tokens from QNLI dataset:\n",
      "['as', 'of', 'that', 'day', ',', 'the', 'new', 'constitution', 'heralding', 'the']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import csv\n",
    "\n",
    "# Download NLTK data if necessary\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Define a function to read and preprocess a TSV dataset\n",
    "def preprocess_tsv_dataset(file_path, column_name):\n",
    "    tokens = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as tsvfile:\n",
    "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            text = row[column_name].lower()  # Convert to lowercase\n",
    "            tokens.extend(word_tokenize(text))  # Tokenize the text and extend the token list\n",
    "    return tokens\n",
    "\n",
    "# File paths to your GLUE datasets and the column name to use\n",
    "sst_path = 'train.tsv'\n",
    "qnli_path = 'dev.tsv'\n",
    "column_name_sst = 'sentence'\n",
    "column_name_qnli = 'sentence'\n",
    "\n",
    "# Preprocess and tokenize the datasets\n",
    "sst_tokens = preprocess_tsv_dataset(sst_path, column_name_sst)\n",
    "qnli_tokens = preprocess_tsv_dataset(qnli_path, column_name_qnli)\n",
    "\n",
    "# Print the first 10 tokens from each dataset\n",
    "print(\"First 10 tokens from SST dataset:\")\n",
    "print(sst_tokens[:10])\n",
    "\n",
    "print(\"\\nFirst 10 tokens from QNLI dataset:\")\n",
    "print(qnli_tokens[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55390c23",
   "metadata": {},
   "source": [
    "**PROBLEM 2**\n",
    "\n",
    "• Write a python function that creates a probability distribution from a list of tokens. This function should return a dictionary that maps a token to a probability (I.e., maps a string to a floating-point value)\n",
    "\n",
    "• Apply your function to the list created in Problem 1 to create SST and QNLI distributions.\n",
    "\n",
    "• Show that both probability distributions sum to 1, allowing for some small numerical rounding error. Or, if they do not, add a comment in your notebook to explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7636220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_probability_distribution(tokens):\n",
    "    total_tokens = len(tokens)\n",
    "    token_counts = {}\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in token_counts:\n",
    "            token_counts[token] += 1\n",
    "        else:\n",
    "            token_counts[token] = 1\n",
    "    \n",
    "    probability_distribution = {}\n",
    "    \n",
    "    for token, count in token_counts.items():\n",
    "        probability = count / total_tokens\n",
    "        probability_distribution[token] = probability\n",
    "    \n",
    "    return probability_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "740bfa60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both probability distributions sum to approximately 1.\n"
     ]
    }
   ],
   "source": [
    "sst_distribution = create_probability_distribution(sst_tokens)\n",
    "qnli_distribution = create_probability_distribution(qnli_tokens)\n",
    "\n",
    "# Calculate the sum of probabilities for each distribution\n",
    "sst_sum = sum(sst_distribution.values())\n",
    "qnli_sum = sum(qnli_distribution.values())\n",
    "\n",
    "# Check if both distributions sum to approximately 1\n",
    "epsilon = 1e-6  # A small epsilon to account for rounding errors\n",
    "\n",
    "if abs(sst_sum - 1) < epsilon and abs(qnli_sum - 1) < epsilon:\n",
    "    print(\"Both probability distributions sum to approximately 1.\")\n",
    "else:\n",
    "    print(\"The probability distributions do not sum to 1 due to numerical rounding errors.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0442262",
   "metadata": {},
   "source": [
    "**PROBLEM 3**\n",
    "\n",
    "•Write a python function that computes the entropy of a random variable, input as a probability distribution.\n",
    "\n",
    "• Use this function to compute the word-level entropy of SST and QNLI, using the distributions you created in Problem 2. Show results in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c926f62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_entropy(probability_distribution):\n",
    "    entropy = 0.0\n",
    "    for probability in probability_distribution.values():\n",
    "        if probability > 0:\n",
    "            entropy -= probability * math.log2(probability)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3251a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-level entropy of SST: 10.079162530566823\n",
      "Word-level entropy of QNLI: 10.056278588664085\n"
     ]
    }
   ],
   "source": [
    "sst_entropy = compute_entropy(sst_distribution)\n",
    "qnli_entropy = compute_entropy(qnli_distribution)\n",
    "\n",
    "print(\"Word-level entropy of SST:\", sst_entropy)\n",
    "print(\"Word-level entropy of QNLI:\", qnli_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696ecb7b",
   "metadata": {},
   "source": [
    "**PROBLEM 4**\n",
    "\n",
    "• Write a python function to compute the KL divergence between two probability distributions.\n",
    "\n",
    "• Apply this function to the distributions you created in Problem 2 to show that KL divergence is not symmetric. [This is also question 2.12 of M&S, p79]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f25e68b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_kl_divergence(p, q, smoothing=1e-6):\n",
    "    kl_divergence = 0.0\n",
    "    for token, probability_p in p.items():\n",
    "        probability_q = q.get(token, smoothing)  # Add smoothing to avoid division by zero\n",
    "        if probability_p > 0:\n",
    "            kl_divergence += probability_p * math.log2(probability_p / probability_q)\n",
    "    return kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2304f536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KL Divergence (SST to QNLI): 1.9005802911305951\n",
      "KL Divergence (QNLI to SST): 1.8128388732656786\n"
     ]
    }
   ],
   "source": [
    "# Compute KL divergence with smoothing\n",
    "kl_sst_to_qnli = compute_kl_divergence(sst_distribution, qnli_distribution)\n",
    "kl_qnli_to_sst = compute_kl_divergence(qnli_distribution, sst_distribution)\n",
    "\n",
    "print(\"KL Divergence (SST to QNLI):\", kl_sst_to_qnli)\n",
    "print(\"KL Divergence (QNLI to SST):\", kl_qnli_to_sst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae19b1df",
   "metadata": {},
   "source": [
    "**PROBLEM 5**\n",
    "\n",
    "• Write a python function that computes the per-word entropy rate of a message relative to a specific probability distribution.\n",
    "\n",
    "• Find a recent movie review online (any website) and compute the entropy rates of this movie review using the distributions you created for both SST and QNLI datasets. Show results in your notebook.\n",
    "\n",
    "\n",
    "For this problem I have used a review from \"Gran Turismo\". \n",
    "https://www.rottentomatoes.com/m/gran_turismo_based_on_a_true_story/reviews?intcmp=rt-what-to-know_read-critics-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5065fa89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_per_word_entropy_rate(message, probability_distribution):\n",
    "    tokens = message.split()  # Tokenize the message by splitting on spaces\n",
    "    total_entropy = 0.0\n",
    "    \n",
    "    for token in tokens:\n",
    "        probability = probability_distribution.get(token, 1e-6)  # Add smoothing to avoid division by zero\n",
    "        if probability > 0:\n",
    "            total_entropy += math.log2(probability)\n",
    "    \n",
    "    num_tokens = len(tokens)\n",
    "    if num_tokens == 0:\n",
    "        return 0.0  # Return 0 entropy if there are no tokens in the message\n",
    "    \n",
    "    entropy_rate = -total_entropy / num_tokens\n",
    "    return entropy_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "017fd912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-word Entropy Rate (SST distribution): 12.84171092944692\n",
      "Per-word Entropy Rate (QNLI distribution): 13.616919515501094\n"
     ]
    }
   ],
   "source": [
    "movie_review = \"I liked the story line. It was inspirational. I liked the family dynamics. The family was very caring and loving towards each other. I liked that the main character persevered and overcame all the challenges and obstacles that confronted him, with help from his coach and mentor. It was a very entertaining movie.\"\n",
    "\n",
    "# Compute per-word entropy rates\n",
    "sst_entropy_rate = compute_per_word_entropy_rate(movie_review, sst_distribution)\n",
    "qnli_entropy_rate = compute_per_word_entropy_rate(movie_review, qnli_distribution)\n",
    "\n",
    "print(\"Per-word Entropy Rate (SST distribution):\", sst_entropy_rate)\n",
    "print(\"Per-word Entropy Rate (QNLI distribution):\", qnli_entropy_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc7bcc6",
   "metadata": {},
   "source": [
    "Handling “zero probabilities”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d759fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def compute_per_word_entropy_rate(message, distribution, vocabulary_size):\n",
    "    # Tokenize the message into words\n",
    "    words = message.split()\n",
    "\n",
    "    # Calculate the entropy rate with Laplace smoothing\n",
    "    entropy = 0.0\n",
    "    for word in words:\n",
    "        # Calculate smoothed probability using Laplace smoothing\n",
    "        p_word = (distribution.get(word, 0) + 1) / (len(words) + vocabulary_size)  # Add-one smoothing\n",
    "\n",
    "        entropy -= math.log2(p_word)  # Using base 2 logarithm\n",
    "\n",
    "    # Normalize the entropy by the number of words\n",
    "    entropy_rate = entropy / len(words)\n",
    "    return entropy_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12a1fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-word Entropy Rate (SST distribution): 13.848587636582906\n",
      "Per-word Entropy Rate (QNLI distribution): 13.962055312359407\n"
     ]
    }
   ],
   "source": [
    "movie_review = \"I liked the story line. It was inspirational. I liked the family dynamics. The family was very caring and loving towards each other. I liked that the main character persevered and overcame all the challenges and obstacles that confronted him, with help from his coach and mentor. It was a very entertaining movie.\"\n",
    "\n",
    "# Define vocabulary sizes for SST and QNLI distributions\n",
    "sst_vocabulary_size = len(sst_distribution)\n",
    "qnli_vocabulary_size = len(qnli_distribution)\n",
    "\n",
    "# Compute per-word entropy rates\n",
    "sst_entropy_rate = compute_per_word_entropy_rate(movie_review, sst_distribution, sst_vocabulary_size)\n",
    "qnli_entropy_rate = compute_per_word_entropy_rate(movie_review, qnli_distribution, qnli_vocabulary_size)\n",
    "\n",
    "print(\"Per-word Entropy Rate (SST distribution):\", sst_entropy_rate)\n",
    "print(\"Per-word Entropy Rate (QNLI distribution):\", qnli_entropy_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
