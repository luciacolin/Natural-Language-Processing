{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc10e8e",
   "metadata": {},
   "source": [
    "<img src=\"logoiit.png\" width=\"200\" img style=\"float: right;\"> \n",
    "\n",
    "**NATURAL LANGUAGE PROCESSING. HOMEWORK 2.**<br>\n",
    "Author: Lucía Colín Cosano. A20552447."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7290bf1",
   "metadata": {},
   "source": [
    "**PROBLEM 1**\n",
    "\n",
    "- Using Python, read in the 2 clickbait datasets, and combine both into a single, shuffled dataset.\n",
    "- Next, split your dataset into train, test, and validation datasets. Use a split of 72% train, 8% validation, and 20% test. (Which is equivalent to a 20% test set, and the remainer split 90%/10% for train and validation).\n",
    "- What is the \"target rate\" of each of these three datasets? That is, what % of the test dataset is labeled as clickbait? Show your result in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f8f5379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb0f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Rate for Training Data: 0.3566026759744037\n",
      "Target Rate for Validation Data: 0.27225130890052357\n",
      "Target Rate for Test Data: 0.3117154811715481\n"
     ]
    }
   ],
   "source": [
    "positive_file_path = r\"C:\\Users\\lulac\\Desktop\\Chicago\\Fall 2023\\Natural Language Processing\\Homework 2\\clickbait.txt\"\n",
    "negative_file_path = r\"C:\\Users\\lulac\\Desktop\\Chicago\\Fall 2023\\Natural Language Processing\\Homework 2\\not-clickbait.txt\"\n",
    "\n",
    "positive_data = []\n",
    "negative_data = []\n",
    "\n",
    "# Read the positive (clickbait) dataset\n",
    "with open(positive_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Check if the line is not empty\n",
    "        if line.strip():\n",
    "            positive_data.append(line.strip())\n",
    "\n",
    "# Read the negative dataset\n",
    "with open(negative_file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        # Check if the line is not empty\n",
    "        if line.strip():\n",
    "            negative_data.append(line.strip())\n",
    "\n",
    "# Creation of dataFrames from the lists\n",
    "positive_df = pd.DataFrame({'text': positive_data, 'label': 'clickbait'})\n",
    "negative_df = pd.DataFrame({'text': negative_data, 'label': 'no clickbait'})\n",
    "\n",
    "\n",
    "# Combination and shuffle of the dataset\n",
    "combined_data = pd.concat([positive_df, negative_df])\n",
    "combined_data = shuffle(combined_data, random_state=42)\n",
    "combined_data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "train_ratio = 0.72\n",
    "validation_ratio = 0.08\n",
    "test_ratio = 0.20\n",
    "\n",
    "total_samples = len(combined_data)\n",
    "train_size = int(train_ratio * total_samples)\n",
    "validation_size = int(validation_ratio * total_samples)\n",
    "test_size = int(test_ratio * total_samples)\n",
    "\n",
    "train_data = combined_data[:train_size]\n",
    "validation_data = combined_data[train_size:(train_size + validation_size)]\n",
    "test_data = combined_data[(train_size + validation_size):]\n",
    "\n",
    "# Calculate the target rate for each dataset\n",
    "target_rate_train = (train_data['label'] == 'clickbait').mean()\n",
    "target_rate_validation = (validation_data['label'] == 'clickbait').mean()\n",
    "target_rate_test = (test_data['label'] == 'clickbait').mean()\n",
    "\n",
    "# Print the target rates\n",
    "print(\"Target Rate for Training Data:\", target_rate_train)\n",
    "print(\"Target Rate for Validation Data:\", target_rate_validation)\n",
    "print(\"Target Rate for Test Data:\", target_rate_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5280abc",
   "metadata": {},
   "source": [
    "**PROBLEM 3** \n",
    "\n",
    "- Using scikit-learn pipelines module, create a Pipeline to train a BOW naïve bayes model. We suggest the classes CountVectorizer and MultinomialNB. Include both unigrams and bigrams in your model in your vectorizer vocabulary\n",
    "- Fit your classifier on your training set\n",
    "- Compute the precision, recall, and F1-score on both your training and validation datasets using functions in sklearn.metrics. Use \"clickbait\" is your target class (I.e., y=1 for clickbait and y=0 for non-clickbait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32780b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38209d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target\n",
    "y_train = (train_data['label'] == 'clickbait').astype(int)\n",
    "y_validation = (validation_data['label'] == 'clickbait').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f39a2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer(ngram_range=(1, 2))),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(ngram_range=(1, 2))</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(ngram_range=(1, 2))),  # Include unigrams and bigrams\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "pipeline.fit(train_data['text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff1386a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Metrics:\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-score: 1.00\n",
      "\n",
      "Validation Metrics:\n",
      "Precision: 0.94\n",
      "Recall: 0.90\n",
      "F1-score: 0.92\n"
     ]
    }
   ],
   "source": [
    "# Predictions on both training and validation datasets\n",
    "y_train_pred = pipeline.predict(train_data['text'])\n",
    "y_validation_pred = pipeline.predict(validation_data['text'])\n",
    "\n",
    "# Compute precision, recall, and F1-score on the training dataset\n",
    "precision_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "# Compute precision, recall, and F1-score on the validation dataset\n",
    "precision_validation = precision_score(y_validation, y_validation_pred)\n",
    "recall_validation = recall_score(y_validation, y_validation_pred)\n",
    "f1_validation = f1_score(y_validation, y_validation_pred)\n",
    "\n",
    "# Print the results\n",
    "print(\"Training Metrics:\")\n",
    "print(f\"Precision: {precision_train:.2f}\")\n",
    "print(f\"Recall: {recall_train:.2f}\")\n",
    "print(f\"F1-score: {f1_train:.2f}\")\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(f\"Precision: {precision_validation:.2f}\")\n",
    "print(f\"Recall: {recall_validation:.2f}\")\n",
    "print(f\"F1-score: {f1_validation:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3248b0b",
   "metadata": {},
   "source": [
    "**PROBLEM 4**\n",
    "\n",
    "Using the ParameterGrid class, run a small grid search where you vary at least 3 parameters of your model\n",
    "- max_df for your count vectorizer (threshold to filter document frequency)\n",
    "- alpha or smoothing of your NaïveBayes model\n",
    "- One other parameter of your choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d22d6aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29d0e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameter Combination:\n",
      "{'classifier__alpha': 0.5, 'vectorizer__max_df': 0.5, 'vectorizer__ngram_range': (1, 1)}\n",
      "Best F1-score: 0.9215686274509804\n"
     ]
    }
   ],
   "source": [
    "# Define the hyperparameter grid for your pipeline\n",
    "param_grid = {\n",
    "    'vectorizer__max_df': [0.5, 0.7, 0.9], \n",
    "    'classifier__alpha': [0.1, 0.5, 1.0], \n",
    "    'vectorizer__ngram_range': [(1, 1), (1, 2)]\n",
    "}\n",
    "\n",
    "# Initialize an empty list to store results\n",
    "results = []\n",
    "\n",
    "# Loop over the parameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    # Create a scikit-learn pipeline with the current parameter settings\n",
    "    pipeline = Pipeline([\n",
    "        ('vectorizer', CountVectorizer(max_df=params['vectorizer__max_df'], ngram_range=params['vectorizer__ngram_range'])),\n",
    "        ('classifier', MultinomialNB(alpha=params['classifier__alpha']))\n",
    "    ])\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    pipeline.fit(train_data['text'], y_train)\n",
    "\n",
    "    # Make predictions on the validation data\n",
    "    y_validation_pred = pipeline.predict(validation_data['text'])\n",
    "\n",
    "    # Calculate the F1-score for the current parameter settings\n",
    "    f1 = f1_score(y_validation, y_validation_pred)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({\n",
    "        'params': params,\n",
    "        'f1_score': f1\n",
    "    })\n",
    "\n",
    "# Find the best parameter combination based on F1-score\n",
    "best_result = max(results, key=lambda x: x['f1_score'])\n",
    "\n",
    "# Print the best parameter combination and its F1-score\n",
    "print(\"Best Parameter Combination:\")\n",
    "print(best_result['params'])\n",
    "print(\"Best F1-score:\", best_result['f1_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ac75",
   "metadata": {},
   "source": [
    "**PROBLEM 5**\n",
    "\n",
    "Using these validation-set metrics from the previous problem, choose one model as your selected model. It is up to you how to choose this model; one approach is to choose the model that shows the highest F1-score on your training set.\n",
    "Next apply your selected model to your test set and compute precision, recall and F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3068ffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = (test_data['label'] == 'clickbait').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84afbe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas en el Conjunto de Prueba:\n",
      "Precision: 0.82\n",
      "Recall: 0.86\n",
      "F1-score: 0.84\n"
     ]
    }
   ],
   "source": [
    "# Find the best parameter combination based on F1-score on the validation set\n",
    "best_result = max(results, key=lambda x: x['f1_score'])\n",
    "best_params = best_result['params']\n",
    "\n",
    "# Create the selected model using the best parameters\n",
    "selected_model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_df=best_params['vectorizer__max_df'], ngram_range=best_params['vectorizer__ngram_range'])),\n",
    "    ('classifier', MultinomialNB(alpha=best_params['classifier__alpha']))\n",
    "])\n",
    "selected_model.fit(train_data['text'], y_train)\n",
    "\n",
    "# Make predictions on the test data using the selected model\n",
    "y_test_pred = selected_model.predict(test_data['text'])\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"Métricas en el Conjunto de Prueba:\")\n",
    "print(f\"Precision: {precision_test:.2f}\")\n",
    "print(f\"Recall: {recall_test:.2f}\")\n",
    "print(f\"F1-score: {f1_test:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15b722",
   "metadata": {},
   "source": [
    "**PROBLEM 6**\n",
    "\n",
    "Using the log-probabilities of the model you selected in the previous problem, select 5 words that are strong Clickbait indicators. That is, if you needed to filter headlines based on a single word, without a machine learning model, then these words would be good options. Show this list of keywords in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f50ae42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Strong Clickbait Indicator Words:\n",
      "the: -3.55\n",
      "you: -4.09\n",
      "to: -4.14\n",
      "this: -4.22\n",
      "is: -4.27\n"
     ]
    }
   ],
   "source": [
    "# Access the classifier from the selected model\n",
    "classifier = selected_model.named_steps['classifier']\n",
    "\n",
    "# Access the feature log probabilities for both classes\n",
    "log_probabilities = classifier.feature_log_prob_\n",
    "\n",
    "# Get the vocabulary used by the vectorizer\n",
    "vectorizer = selected_model.named_steps['vectorizer']\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary to store words and their log probabilities for clickbait\n",
    "clickbait_log_probs = {}\n",
    "non_clickbait_log_probs = {}\n",
    "\n",
    "# Assuming that clickbait is class 1 and non-clickbait is class 0\n",
    "for word_idx, word in enumerate(vocabulary):\n",
    "    clickbait_log_prob = log_probabilities[1][word_idx]\n",
    "    non_clickbait_log_prob = log_probabilities[0][word_idx]\n",
    "    \n",
    "    clickbait_log_probs[word] = clickbait_log_prob\n",
    "    non_clickbait_log_probs[word] = non_clickbait_log_prob\n",
    "\n",
    "# Sort the words by their log probabilities for clickbait (in descending order)\n",
    "sorted_clickbait_words = sorted(clickbait_log_probs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Select the top 5 words as strong clickbait indicators\n",
    "top_clickbait_words = sorted_clickbait_words[:5]\n",
    "\n",
    "# Print the list of top clickbait indicator words\n",
    "print(\"Top 5 Strong Clickbait Indicator Words:\")\n",
    "for word, log_prob in top_clickbait_words:\n",
    "    print(f\"{word}: {log_prob:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36c024d",
   "metadata": {},
   "source": [
    "**PROBLEM 7**\n",
    "\n",
    "Your IT department has reached out to you because they heard you can help them find clickbait. They are interested in your machine learning model, but they need a solution today.\n",
    "- Write a regular expression that checks if any of the keywords from the previous problem are found in the text. You should write one regular expression that detects any of your top 5 keywords. Your regular expression should be aware of word boundaries in some way. That is, the keyword \"win\" should not be detected in the text \"Gas prices up in winter months\".\n",
    "- Using the python re library – apply your function to your test set. (See function re.search). What is the precision and recall of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb277286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.4180327868852459\n",
      "Recall: 0.6845637583892618\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Define the top 5 clickbait indicator keywords\n",
    "top_clickbait_keywords = [\"the\", \"you\", \"to\", \"this\", \"is\"]\n",
    "\n",
    "# Create a regular expression pattern to match any of the keywords with word boundaries\n",
    "pattern = r'\\b(?:' + '|'.join(re.escape(keyword) for keyword in top_clickbait_keywords) + r')\\b'\n",
    "\n",
    "# Initialize variables to count true positives, false positives, and false negatives\n",
    "true_positives = 0\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "# Iterate over the test data and labels to calculate precision and recall\n",
    "for text, label in zip(test_data['text'], y_test):\n",
    "    # Search for the pattern in the text\n",
    "    if re.search(pattern, text, flags=re.IGNORECASE):\n",
    "        # If the pattern is found in the text and it's labeled as clickbait, it's a true positive\n",
    "        if label == 1:\n",
    "            true_positives += 1\n",
    "        # If the pattern is found in the text but it's not labeled as clickbait, it's a false positive\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    # If the pattern is not found in the text and it's labeled as clickbait, it's a false negative\n",
    "    elif label == 1:\n",
    "        false_negatives += 1\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = true_positives / (true_positives + false_positives)\n",
    "recall = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "# Print the precision and recall\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
